{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN Implementation and Experiments\n",
    "\n",
    "This notebook demonstrates the RNN implementation and experiments. We'll work with the NusaX-Sentiment dataset to perform text classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append('../..')\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, Model, load_model\n",
    "from tensorflow.keras.layers import Embedding, SimpleRNN, Dropout, Dense, Bidirectional\n",
    "from sklearn.metrics import f1_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.src.models.base_model.utils.nusax_loader import NusaXLoader\n",
    "from src.models.src.models.rnn.experiment import RNNExperiments\n",
    "from src.models.src.models.rnn.rnn_model import RNNModel\n",
    "from src.models.src.models.rnn.rnn_layer import RNNLayer\n",
    "from src.models.src.models.base_model.layers.embedding_layer import EmbeddingLayer\n",
    "from src.models.src.models.base_model.layers.dense_layer import DenseLayer\n",
    "from src.models.src.models.base_model.layers.dropout_layer import DropoutLayer\n",
    "from src.models.src.models.base_model.layers.activation_layer import Softmax\n",
    "from src.models.src.models.base_model.utils.evaluation import compare_keras_vs_scratch\n",
    "from src.models.src.models.base_model.utils.visualization import plot_training_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(f\"Num GPUs Available: {len(gpus)}\")\n",
    "        print(f\"Num Logical GPUs: {len(logical_gpus)}\")\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init data loader\n",
    "data_loader = NusaXLoader(batch_size=32, add=True)\n",
    "# Ini add buat ngubah pathnya nambah \"../\" tapi harusnya gaperlu soalnya di file siblingnsnya juga gapake ini, api entah kenapa gabisa jalan kalo gadipasang\n",
    "\n",
    "# Train\n",
    "train_dataset = data_loader.get_dataset('train')\n",
    "for tokens, labels in train_dataset.take(1):\n",
    "    sample_tokens = tokens.numpy()\n",
    "    sample_labels = labels.numpy()\n",
    "    break\n",
    "\n",
    "# Get vocabulary\n",
    "vocab = data_loader.get_vocabulary()\n",
    "print(f\"Vocabulary size: {len(vocab)}\")\n",
    "print(f\"First 20 words in vocabulary: {vocab[:20]}\")\n",
    "\n",
    "# Output Example\n",
    "print(\"\\nSample texts:\")\n",
    "for i in range(3):\n",
    "    # Convert token IDs back to words\n",
    "    words = [vocab[idx] if idx < len(vocab) else \"[UNK]\" for idx in sample_tokens[i] if idx > 0]\n",
    "    text = \" \".join(words)\n",
    "    print(f\"Text {i+1}: {text}\")\n",
    "    print(f\"Label: {sample_labels[i]}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Hyperparameter Experiments with RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments = RNNExperiments(data_loader=data_loader, batch_size=32, epochs=10, embedding_dim=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Experiment: Number of RNN Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define variants for number of RNN layers\n",
    "layer_count_variants = [\n",
    "    (1, \"1 RNN Layer\"),\n",
    "    (2, \"2 RNN Layers\"),\n",
    "    (3, \"3 RNN Layers\")\n",
    "]\n",
    "\n",
    "# Run experiment\n",
    "layer_count_models, layer_count_histories = experiments.run_layer_count_experiment(layer_count_variants)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Experiment: Number of RNN Cells per Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define variants for cell counts\n",
    "cell_count_variants = [\n",
    "    ([32], \"32 Units\"),\n",
    "    ([64], \"64 Units\"),\n",
    "    ([128], \"128 Units\")\n",
    "]\n",
    "\n",
    "# Run experiment\n",
    "cell_count_models, cell_count_histories = experiments.run_cell_count_experiment(cell_count_variants)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Experiment: RNN Direction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define variants for RNN direction\n",
    "direction_variants = [\n",
    "    (False, \"Unidirectional RNN\"),\n",
    "    (True, \"Bidirectional RNN\")\n",
    "]\n",
    "\n",
    "# Run experiment\n",
    "direction_models, direction_histories = experiments.run_direction_experiment(direction_variants)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. From-Scratch RNN Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose one of the trained models (e.g., from the bidirectional experiment)\n",
    "keras_model_path = \"../../output/models/rnn/rnn_bidirectional.keras\"\n",
    "if os.path.exists(keras_model_path):\n",
    "    keras_model = load_model(keras_model_path)\n",
    "elif len(direction_models) > 1 and direction_models[1][0] is not None:\n",
    "    keras_model = direction_models[1][0]  # Bidirectional RNN model\n",
    "else:\n",
    "    keras_model = direction_models[0][0]  # Unidirectional RNN model\n",
    "\n",
    "# Summary of the chosen model\n",
    "keras_model.summary()\n",
    "\n",
    "# Save the model weights\n",
    "keras_model.save_weights('../output/models/rnn/rnn_model.weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get embedding dimension and vocabulary size\n",
    "embedding_dim = 100\n",
    "vocab_size = len(data_loader.get_vocabulary())\n",
    "hidden_dim = 128\n",
    "num_classes = data_loader.num_classes\n",
    "sequence_length = data_loader.max_sequence_length\n",
    "bidirectional = True  # Set this based on the chosen model\n",
    "\n",
    "# Create a from-scratch RNN model that matches the Keras model\n",
    "scratch_model = RNNModel()\n",
    "\n",
    "# Add layers corresponding to the Keras model architecture\n",
    "scratch_model.add(EmbeddingLayer(input_dim=vocab_size, output_dim=embedding_dim))\n",
    "scratch_model.add(RNNLayer(input_dim=embedding_dim, hidden_dim=hidden_dim, bidirectional=bidirectional, return_sequences=False))\n",
    "scratch_model.add(DropoutLayer(dropout_rate=0.2))\n",
    "scratch_model.add(DenseLayer(input_dim=hidden_dim*2 if bidirectional else hidden_dim, output_dim=num_classes))\n",
    "scratch_model.add(Softmax())\n",
    "\n",
    "# Load weights from the Keras model\n",
    "scratch_model.load_weights_from_keras(keras_model)\n",
    "\n",
    "print(\"Weights loaded from Keras model to from-scratch implementation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get test data\n",
    "x_test, y_test = data_loader.get_vectorized_data('test')\n",
    "\n",
    "# Compare predictions\n",
    "comparison = compare_keras_vs_scratch(keras_model, scratch_model, x_test, y_test, batch_size=32)\n",
    "\n",
    "print(\"\\nKeras Model Metrics:\")\n",
    "print(f\"Accuracy: {comparison['keras_metrics']['accuracy']:.4f}\")\n",
    "print(f\"Macro F1-Score: {comparison['keras_metrics']['macro_f1']:.4f}\")\n",
    "\n",
    "print(\"\\nFrom-Scratch Model Metrics:\")\n",
    "print(f\"Accuracy: {comparison['scratch_metrics']['accuracy']:.4f}\")\n",
    "print(f\"Macro F1-Score: {comparison['scratch_metrics']['macro_f1']:.4f}\")\n",
    "\n",
    "print(f\"\\nModel Agreement: {comparison['model_agreement']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a few test samples\n",
    "num_samples = 5\n",
    "sample_indices = np.random.choice(x_test.shape[0], num_samples, replace=False)\n",
    "sample_texts = x_test[sample_indices]\n",
    "sample_labels = y_test[sample_indices]\n",
    "\n",
    "# Make predictions with both models\n",
    "keras_preds = np.argmax(keras_model.predict(sample_texts), axis=1)\n",
    "scratch_preds = scratch_model.predict(sample_texts)\n",
    "\n",
    "# Define sentiment labels\n",
    "sentiment_labels = [\"Negative\", \"Neutral\", \"Positive\"]\n",
    "\n",
    "# Visualize the results\n",
    "for i in range(num_samples):\n",
    "    # Convert token IDs back to words\n",
    "    words = [vocab[idx] if idx < len(vocab) else \"[UNK]\" for idx in sample_texts[i] if idx > 0]\n",
    "    text = \" \".join(words)\n",
    "    \n",
    "    # Show true label and predictions\n",
    "    keras_correct = keras_preds[i] == sample_labels[i]\n",
    "    scratch_correct = scratch_preds[i] == sample_labels[i]\n",
    "    \n",
    "    print(f\"\\nText: {text[:100]}{'...' if len(text) > 100 else ''}\")\n",
    "    print(f\"True sentiment: {sentiment_labels[sample_labels[i]]}\")\n",
    "    print(f\"Keras prediction: {sentiment_labels[keras_preds[i]]} {'✓' if keras_correct else '✗'}\")\n",
    "    print(f\"Scratch prediction: {sentiment_labels[scratch_preds[i]]} {'✓' if scratch_correct else '✗'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Additional Experiments: Multi-layer RNN Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure output directories exist\n",
    "os.makedirs(\"../../output/models/rnn\", exist_ok=True)\n",
    "os.makedirs(\"../../output/results/rnn\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load models from previously saved files\n",
    "model_paths = {\n",
    "    \"1_layer\": \"../output/models/rnn/rnn_layers_1.keras\",\n",
    "    \"2_layer\": \"../output/models/rnn/rnn_layers_2.keras\",\n",
    "    \"3_layer\": \"../output/models/rnn/rnn_layers_3.keras\"\n",
    "}\n",
    "\n",
    "loaded_models = {}\n",
    "\n",
    "for name, path in model_paths.items():\n",
    "    if os.path.exists(path):\n",
    "        print(f\"Loading model: {name}\")\n",
    "        loaded_models[name] = load_model(path)\n",
    "        loaded_models[name].summary()\n",
    "    else:\n",
    "        print(f\"Model file not found: {path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare the multi-layer models\n",
    "results = {}\n",
    "\n",
    "for name, path in model_paths.items():\n",
    "    if not os.path.exists(path):\n",
    "        print(f\"Model file not found: {path}\")\n",
    "        continue\n",
    "        \n",
    "    print(f\"\\nBuilding and comparing {name} model...\")\n",
    "    \n",
    "    # Create a matching from-scratch model for each Keras model\n",
    "    try:\n",
    "        comparison_result = experiments.compare_models(keras_model_path=path)\n",
    "        \n",
    "        # Store the results\n",
    "        results[name] = comparison_result\n",
    "        \n",
    "        # Print key metrics\n",
    "        print(\"\\nComparison Results:\")\n",
    "        print(f\"Keras Accuracy: {comparison_result['keras_metrics']['accuracy']:.4f}\")\n",
    "        print(f\"Scratch Accuracy: {comparison_result['scratch_metrics']['accuracy']:.4f}\")\n",
    "        print(f\"Model Agreement: {comparison_result['model_agreement']:.4f}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing model {name}: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize agreement between Keras and from-scratch models\n",
    "if results:\n",
    "    model_names = list(results.keys())\n",
    "    agreements = [results[name]['model_agreement'] for name in model_names]\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(model_names, agreements, color='skyblue')\n",
    "    plt.title('Agreement Between Keras and From-Scratch Models')\n",
    "    plt.xlabel('Model')\n",
    "    plt.ylabel('Agreement')\n",
    "    plt.ylim(0, 1.0)\n",
    "    plt.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Add agreement values on top of bars\n",
    "    for i, v in enumerate(agreements):\n",
    "        plt.text(i, v + 0.01, f'{v:.4f}', ha='center')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('../../output/results/rnn/model_agreement.png')\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
