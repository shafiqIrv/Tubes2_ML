{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-28 15:30:28.740960: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-05-28 15:30:28.758901: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1748421028.776826   45142 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1748421028.781646   45142 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1748421028.796537   45142 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1748421028.796560   45142 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1748421028.796562   45142 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1748421028.796563   45142 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-05-28 15:30:28.802252: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from sklearn.metrics import f1_score\n",
    "from models.base_model.layers.activation_layer import ReLU, Softmax\n",
    "from models.base_model.layers.dense_layer import DenseLayer\n",
    "from models.base_model.layers.conv_2d_layer import Conv2DLayer\n",
    "from models.base_model.layers.max_pooling_2d_layer import MaxPooling2DLayer\n",
    "from models.base_model.layers.average_pooling_2d_layer import AveragePooling2DLayer\n",
    "from models.base_model.layers.flatten_layer import FlattenLayer\n",
    "from models.cnn.cnn import CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: (40000, 32, 32, 3)\n",
      "Validation set: (10000, 32, 32, 3)\n",
      "Test set: (10000, 32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "# Load CIFAR-10\n",
    "(x_train_full, y_train_full), (x_test, y_test) = (\n",
    "    keras.datasets.cifar10.load_data()\n",
    ")\n",
    "\n",
    "# Normalize pixel values\n",
    "x_train_full = x_train_full.astype(\"float32\") / 255.0\n",
    "x_test = x_test.astype(\"float32\") / 255.0\n",
    "\n",
    "# Create train/validation split (40k train, 10k validation)\n",
    "split_idx = 40000\n",
    "\n",
    "x_train = x_train_full[:split_idx]\n",
    "y_train = y_train_full[:split_idx].flatten()\n",
    "x_val = x_train_full[split_idx:]\n",
    "y_val = y_train_full[split_idx:].flatten()\n",
    "x_test = x_test\n",
    "y_test = y_test.flatten()\n",
    "\n",
    "print(f\"Training set: {x_train.shape}\")\n",
    "print(f\"Validation set: {x_val.shape}\")\n",
    "print(f\"Test set: {x_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_keras_model(\n",
    "    filters_list=[32, 64, 128], kernel_sizes=[3, 3, 3], pooling_type=\"max\"\n",
    "):\n",
    "    model = keras.Sequential()\n",
    "\n",
    "    # Add convolutional layers\n",
    "    for i, (filters, kernel_size) in enumerate(zip(filters_list, kernel_sizes)):\n",
    "        if i == 0:\n",
    "            model.add(\n",
    "                keras.layers.Conv2D(\n",
    "                    filters, kernel_size, activation=\"relu\", input_shape=(32, 32, 3), padding=\"same\"\n",
    "                )\n",
    "            )\n",
    "        else:\n",
    "            model.add(keras.layers.Conv2D(filters, kernel_size, activation=\"relu\", padding=\"same\"))\n",
    "\n",
    "        # Add pooling layer\n",
    "        if pooling_type == \"max\":\n",
    "            model.add(keras.layers.MaxPooling2D((2, 2)))\n",
    "        else:\n",
    "            model.add(keras.layers.AveragePooling2D((2, 2)))\n",
    "\n",
    "    # Add dense layers\n",
    "    model.add(keras.layers.Flatten())\n",
    "    model.add(keras.layers.Dense(64, activation=\"relu\"))\n",
    "    model.add(keras.layers.Dense(10, activation=\"softmax\"))\n",
    "\n",
    "    return model\n",
    "\n",
    "def train_keras_model(model, epochs=10):\n",
    "    model.compile(\n",
    "        optimizer=\"adam\",\n",
    "        loss=\"sparse_categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\"],\n",
    "    )\n",
    "\n",
    "    history = model.fit(\n",
    "        x_train,\n",
    "        y_train,\n",
    "        batch_size=32,\n",
    "        epochs=epochs,\n",
    "        validation_data=(x_val, y_val),\n",
    "        verbose=1,\n",
    "    )\n",
    "\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_scratch_model(\n",
    "    filters_list=[32, 64, 128], kernel_sizes=[3, 3, 3], pooling_type=\"max\"\n",
    "):\n",
    "    cnn = CNN(input_shape=(32, 32, 3), num_classes=10)\n",
    "\n",
    "    # Add convolutional layers\n",
    "    for _, (filters, kernel_size) in enumerate(zip(filters_list, kernel_sizes)):\n",
    "        cnn.add(\n",
    "            Conv2DLayer(filters=filters, kernel_size=kernel_size, activation=ReLU(), padding=\"same\")\n",
    "        )\n",
    "\n",
    "        if pooling_type == \"max\":\n",
    "            cnn.add(MaxPooling2DLayer(pool_size=(2, 2)))\n",
    "        else:\n",
    "            cnn.add(AveragePooling2DLayer(pool_size=(2, 2)))\n",
    "\n",
    "    # Add dense layers\n",
    "    cnn.add(FlattenLayer())\n",
    "    cnn.add(DenseLayer(input_dim=None, output_dim=64, activation=ReLU()))\n",
    "    cnn.add(DenseLayer(input_dim=64, output_dim=10, activation=Softmax()))\n",
    "\n",
    "    return cnn\n",
    "\n",
    "def calculate_dense_input_dim(scratch_model, sample_input):\n",
    "    # Forward pass through conv layers only\n",
    "    output = sample_input\n",
    "\n",
    "    conv_layer_count = 0\n",
    "    for layer in scratch_model.layers:\n",
    "        if isinstance(\n",
    "            layer, (Conv2DLayer, MaxPooling2DLayer, AveragePooling2DLayer)\n",
    "        ):\n",
    "            output = layer.forward(output)\n",
    "            conv_layer_count += 1\n",
    "        elif isinstance(layer, FlattenLayer):\n",
    "            output = layer.forward(output)\n",
    "            # Update the first dense layer's input dimension\n",
    "            for next_layer in scratch_model.layers[conv_layer_count + 1 :]:\n",
    "                if (\n",
    "                    isinstance(next_layer, DenseLayer)\n",
    "                    and next_layer.input_dim is None\n",
    "                ):\n",
    "                    next_layer.input_dim = output.shape[1]\n",
    "                    next_layer.weights = (\n",
    "                        np.random.randn(next_layer.input_dim, next_layer.output_dim)\n",
    "                        * 0.01\n",
    "                    )\n",
    "                    break\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sandwicheese/Projects/Tubes2_ML/.venv/lib/python3.10/site-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
      "I0000 00:00:1748421034.756458   45142 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 3586 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1748421037.062598   45283 service.cc:152] XLA service 0x7f8b20015c60 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1748421037.062646   45283 service.cc:160]   StreamExecutor device (0): NVIDIA GeForce RTX 3060 Laptop GPU, Compute Capability 8.6\n",
      "2025-05-28 15:30:37.087358: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "I0000 00:00:1748421037.267217   45283 cuda_dnn.cc:529] Loaded cuDNN version 90300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m  43/1250\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.1365 - loss: 2.2782"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1748421039.436638   45283 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - accuracy: 0.3617 - loss: 1.7297 - val_accuracy: 0.5715 - val_loss: 1.2100\n",
      "Epoch 2/10\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - accuracy: 0.6068 - loss: 1.1152 - val_accuracy: 0.6395 - val_loss: 1.0126\n",
      "Epoch 3/10\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - accuracy: 0.6899 - loss: 0.8912 - val_accuracy: 0.6946 - val_loss: 0.8789\n",
      "Epoch 4/10\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.7302 - loss: 0.7772 - val_accuracy: 0.6993 - val_loss: 0.8612\n",
      "Epoch 5/10\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - accuracy: 0.7622 - loss: 0.6763 - val_accuracy: 0.7268 - val_loss: 0.8019\n",
      "Epoch 6/10\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - accuracy: 0.7948 - loss: 0.5911 - val_accuracy: 0.7352 - val_loss: 0.7999\n",
      "Epoch 7/10\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - accuracy: 0.8197 - loss: 0.5148 - val_accuracy: 0.7351 - val_loss: 0.8018\n",
      "Epoch 8/10\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - accuracy: 0.8388 - loss: 0.4561 - val_accuracy: 0.7335 - val_loss: 0.8687\n",
      "Epoch 9/10\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - accuracy: 0.8647 - loss: 0.3879 - val_accuracy: 0.7320 - val_loss: 0.8729\n",
      "Epoch 10/10\n",
      "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.8834 - loss: 0.3314 - val_accuracy: 0.7263 - val_loss: 0.9360\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "Keras F1-Score: 0.7184\n",
      "Scratch F1-Score: 0.7184\n",
      "Agreement: 100.0%\n"
     ]
    }
   ],
   "source": [
    "# Create and train Keras model\n",
    "keras_model = create_keras_model()\n",
    "history = train_keras_model(keras_model)\n",
    "\n",
    "# Evaluate Keras model\n",
    "keras_pred = keras_model.predict(x_test[:1000])\n",
    "keras_classes = np.argmax(keras_pred, axis=1)\n",
    "keras_f1 = f1_score(y_test[:1000], keras_classes, average=\"macro\")\n",
    "\n",
    "# Create corresponding scratch model\n",
    "scratch_model = create_scratch_model()\n",
    "\n",
    "# Calculate dense layer input dimensions\n",
    "sample_input = x_test[:1]\n",
    "calculate_dense_input_dim(scratch_model, sample_input)\n",
    "\n",
    "# Load weights from Keras to scratch model\n",
    "scratch_model.load_weights_from_keras(keras_model)\n",
    "\n",
    "# Test scratch model\n",
    "scratch_pred = scratch_model.predict(x_test[:1000])\n",
    "scratch_f1 = f1_score(y_test[:1000], scratch_pred, average=\"macro\")\n",
    "\n",
    "# Calculate agreement\n",
    "keras_subset = np.argmax(keras_model.predict(x_test[:1000]), axis=1)\n",
    "agreement = np.mean(keras_subset == scratch_pred) * 100\n",
    "\n",
    "print(f\"Keras F1-Score: {keras_f1:.4f}\")\n",
    "print(f\"Scratch F1-Score: {scratch_f1:.4f}\")\n",
    "print(f\"Agreement: {agreement:.1f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
